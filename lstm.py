# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xCfE6sUNC-oF3ZGmS-rfm46T7G2BfguX
"""

import numpy as np
from rnn import RNN
import tensorflow as tf
from keras.layers import Embedding, Dense, Layer, LSTM, Input, RNN
from tensorflow.keras.optimizers import Adam
from keras.models import Model
import keras.backend as K
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class LSTMCell(Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(LSTMCell, self).__init__(**kwargs)
        #implementation = kwargs.pop('implementation', 1)

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.kernel = self.add_weight(
            shape=(input_dim, self.units * 4), name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units * 4),name='recurrent_kernel')
        self.bias = self.add_weight(shape=(self.units * 4), name='bias')
        self.built = True
    
    def _compute_carry_and_output(self, x, h_tm1, c_tm1):
      #"""Computes carry and output using split kernels."""
      x = x_i, x_f, x_c, x_o 
      h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1
      i = self.recurrent_activation(
        x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))
      f = self.recurrent_activation(x_f + backend.dot(
        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
      c = f * c_tm1 + i * self.activation(x_c + backend.dot(
        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))
      o = self.recurrent_activation(
          x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))
      return c, o

    
    def call(self, inputs, states, training=None):
      h_tm1 = states[0]  # previous memory state
      c_tm1 = states[1]  # previous carry state

      if self.implementation == 1:
        inputs_i = inputs
        inputs_f = inputs
        inputs_c = inputs
        inputs_o = inputs

        k_i, k_f, k_c, k_o = tf.split(self.kernel, num_or_size_splits=4, axis=1)
        x_i = K.dot(inputs_i, k_i)
        x_f = K.dot(inputs_f, k_f)
        x_c = K.dot(inputs_c, k_c)
        x_o = K.dot(inputs_o, k_o)

      if self.use_bias:
        b_i, b_f, b_c, b_o = tf.split(
            self.bias, num_or_size_splits=4, axis=0)
        x_i = K.bias_add(x_i, b_i)
        x_f = K.bias_add(x_f, b_f)
        x_c = K.bias_add(x_c, b_c)
        x_o = K.bias_add(x_o, b_o)

      x = (x_i, x_f, x_c, x_o)
      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)
      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)

      h = o * self.activation(c)
      return h, [h, c]



class LSTM(RNN):
    def __init__(self, units,
                 activation='tanh',
                 recurrent_activation='hard_sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 unit_forget_bias=True,
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 dropout=None,
                 recurrent_dropout=None,
                 implementation=1,
                 return_sequences=False,
                 return_state=False,
                 go_backwards=False,
                 stateful=True,
                 unroll=True,
                 **kwargs):

        cell = LSTMCell(units)
        super(LSTM, self).__init__(cell,
                                   return_sequences=return_sequences,
                                   return_state=return_state,
                                   go_backwards=go_backwards,
                                   stateful=stateful,
                                   unroll=unroll,
                                   **kwargs)
        #self.get_input_size() = input_size

    def call(self, inputs, mask=None, training=None, initial_state=None):
        return super(LSTM, self).call(inputs,
                                      mask=mask,
                                      training=training,
                                      initial_state=initial_state)