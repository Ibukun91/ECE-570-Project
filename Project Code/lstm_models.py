# -*- coding: utf-8 -*-
"""lstm_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wW6oT1N_NUIWA7ZARYrnsrtwqNBbgr1f
"""

"""Cartpole DQN models."""
import tensorflow as tf
from keras.layers import Embedding, Dense, Layer, LSTM, Input, RNN
from tensorflow.keras.optimizers import Adam
from keras.models import Model
import keras.backend as K
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class LSTMCell(Layer):

    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(LSTMCell, self).__init__(**kwargs)
        implementation = kwargs.pop('implementation', 1)

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.kernel = self.add_weight(
            shape=(input_dim, self.units * 4), name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units * 4),name='recurrent_kernel')
        self.bias = self.add_weight(shape=(self.units * 4), name='bias')
        self.built = True
    
    def _compute_carry_and_output(self, x, h_tm1, c_tm1):
      #"""Computes carry and output using split kernels."""
      x = x_i, x_f, x_c, x_o 
      h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1
      i = self.recurrent_activation(
        x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))
      f = self.recurrent_activation(x_f + backend.dot(
        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
      c = f * c_tm1 + i * self.activation(x_c + backend.dot(
        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))
      o = self.recurrent_activation(
          x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))
      return c, o

    
    def call(self, inputs, states, training=None):
      h_tm1 = states[0]  # previous memory state
      c_tm1 = states[1]  # previous carry state

      if self.implementation == 1:
        inputs_i = inputs
        inputs_f = inputs
        inputs_c = inputs
        inputs_o = inputs

        k_i, k_f, k_c, k_o = tf.split(self.kernel, num_or_size_splits=4, axis=1)
        x_i = K.dot(inputs_i, k_i)
        x_f = K.dot(inputs_f, k_f)
        x_c = K.dot(inputs_c, k_c)
        x_o = K.dot(inputs_o, k_o)

      if self.use_bias:
        b_i, b_f, b_c, b_o = tf.split(
            self.bias, num_or_size_splits=4, axis=0)
        x_i = K.bias_add(x_i, b_i)
        x_f = K.bias_add(x_f, b_f)
        x_c = K.bias_add(x_c, b_c)
        x_o = K.bias_add(x_o, b_o)

      x = (x_i, x_f, x_c, x_o)
      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)
      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)

      h = o * self.activation(c)
      return h, [h, c]

#from rnn import RNN
batch_size = 1
# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).
# Each input sequence will be of size (28, 28) (height is treated like time).
input_dim = 1

units = 5
output_size = 1  # labels are from 0 to 9

# Build the LSTM model
def build_model():           #allow_cudnn_kernel=True
  """Build a stateful simple LSTM model."""
    # CuDNN is only available at the layer level, and not at the cell level.
    # This means `LSTM(units)` will use the CuDNN kernel,
    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.
            # Wrapping a LSTMCell in a RNN layer will not use CuDNN.
  lstm_layer = keras.layers.RNN(
            keras.layers.LSTMCell(units), input_shape=(None, input_dim)
        )
  model = keras.models.Sequential(
        [
            lstm_layer,
            keras.layers.Dense(output_size),
        ]
    )
  return model
